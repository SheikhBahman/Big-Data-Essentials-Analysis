{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Applications: TF-IDF\n",
    "\n",
    "In this task Hadoop Streaming is used to process Wikipedia articles dump (/data/wiki/en_articles_part).\n",
    "\n",
    "The purpose of this task is to calculate tf*idf for each pair (word, article) from the Wikipedia dump. Apply the stop words filter to speed up calculations. Term frequency (tf) is a function depending on a term (word) and a document (article):\n",
    "\n",
    "tf(term, doc_id) = Nt/N,\n",
    "\n",
    "where Nt - quantity of particular term in the document, N - the total number of terms in the document (without stop words)\n",
    "\n",
    "Inverse document frequency (idf) is a function depends on a term:\n",
    "\n",
    "idf(term) = 1/log(1 + Dt),\n",
    "\n",
    "where Dt - number of documents in the dataset with the particular term.\n",
    "\n",
    "You can find more information here: https://en.wikipedia.xn--org/wiki/Tfidf-q82h but use just the formulas mentioned above.\n",
    "\n",
    "Dataset location: /data/wiki/en_articles_part\n",
    "\n",
    "Stop words list is in ‘/datasets/stop_words_en.txt’ file.\n",
    "\n",
    "Format: article_id <tab> article_text\n",
    "\n",
    "To parse the articles don’t forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities. To cope with Unicode we recommend to use the following tokenizer:\n",
    "\n",
    "Output: tf*idf for term=’labor’ and article_id=12\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "0.00351\n",
    "\n",
    "Hint: all Wikipedia article_ids are greater than 0. So you can use a dummy article_id=0 to calculate the number of documents with each term.\n",
    "\n",
    "Please, ensure that you call the right interpreter (python2 or python3), do not write just \"python\" without the major version because we do not guarantee that always only one particular version is always set as default in the grading system.\n",
    "\n",
    "If you want to deploy the environment on your own machine, please use bigdatateam/yarn-notebook Docker container. New image: bigdatateam/hysh-full:py3-c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "\n",
    "with open('stop_words_en.txt') as f:\n",
    "    stop_words = f.read().split()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        words = [word.lower() for word in words if (word.lower() not in stop_words)]\n",
    "        \n",
    "        words_counter = collections.Counter(words)\n",
    "        words_total = sum(words_counter.values())\n",
    "\n",
    "        for word, count in sorted(words_counter.items()):\n",
    "            if not word.isalpha(): continue\n",
    "            tf = float(count)/float(words_total)            \n",
    "            print(word, article_id, str(tf), sep=\"\\t\")\n",
    "             \n",
    "    except ValueError:        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "current_word = None\n",
    "articles = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, article_id, tf = line.strip().split('\\t', 2)      \n",
    "        if current_word != word:\n",
    "            if current_word:\n",
    "                idf = 1.0 / math.log(1 + len(articles))\n",
    "                for article, tf in articles.items():\n",
    "                    print(current_word, article, str(tf*idf), sep=\"\\t\")            \n",
    "            current_word = word\n",
    "            articles = {}\n",
    "        articles[article_id] = float(tf)    \n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "if current_word:    \n",
    "    idf = 1.0 / math.log(1 + len(articles))\n",
    "    for article, tf in articles.items():\n",
    "        print(current_word, article, str(tf*idf), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "HADOOP_STREAMING_JAR=\"/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "OUT_DIR=\"TFIDF_\"$(date +\"%s%6N\")\n",
    "LOGS=\"stderr_logs.txt\"\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar $HADOOP_STREAMING_JAR \\\n",
    "    -D mapred.jab.name=\"TDIDF\" \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -files mapper.py,reducer.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -numReduceTasks 4 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null 2> $LOGS \n",
    "\n",
    "hdfs dfs -cat ${OUT_DIR}/part* | grep -w \"labor\" | grep -w \"12\" | cut -f 3\n",
    "cat $LOGS >&2\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
