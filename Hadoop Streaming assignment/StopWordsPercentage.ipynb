{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KU56zkC0S80M"
   },
   "source": [
    "# Hadoop Streaming assignment 2: Stop Words\n",
    "\n",
    "Improve the previous program to calculate how many stop words are in the input dataset. Stop words list is in ‘/datasets/stop_words_en.txt’ file. Use Hadoop counter to count the number of stop words and total words in the dataset. The result is the percentage of stop words in the entire dataset (without percent symbol).\n",
    "\n",
    "The result on the sample dataset: 41.603\n",
    "    \n",
    "Hint. As you can see in the Hadoop Streaming userguide \"you will need to use \"-file\" option to tell the framework to pack your executable files as a part of job submission.\". In general you can attach to the job not only executable files and then access them within your mappers and reducers as if were located in the same directory.\n",
    "\n",
    "Hint 2. The solution can contain either one or two Hadoop MapReduce jobs. In each case the last MapReduce job will have either 0 or > 1 reducers.\n",
    "\n",
    "You should extract counters’ values from Hadoop logs after MapReduce jobs completion. You will use them to calculate the result. For doing this it is convenient to write a script. The script should do the following:\n",
    "\n",
    "* read the Hadoop logs from stderr of the last notebook’s cell;\n",
    "* extract the values of the Hadoop counters for: “stop words” and “total words”;\n",
    "* calculate the percentage of stop words;\n",
    "* print this percentage in the correct format to stdout;\n",
    "* print Hadoop logs into stderr. It will be used as the input of your script.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper\n",
    "\n",
    "\n",
    "Hint: Create the mapper, which calculates Total word and Stop word amounts. You may redirect this information to sys.stderr. This will make it possible to parse these data on the next steps.\n",
    "\n",
    "Distributed cache: If we add option -files mapper.py,reducer.py,/datasets/stop_words_en.txt, then mapper.py, reducer.py and stop_words_en.txt file will be in the same directory on the datanodes. Hence, it is necessary to use a relative path stop_words_en.txt from the mapper to access this txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "totalWordsCount = 0\n",
    "stopWordsCount = 0\n",
    "\n",
    "with open('stop_words_en.txt') as f:\n",
    "    stopWords = f.read().split()\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        for word in words:\n",
    "            if word in stopWords:\n",
    "                stopWordsCount += 1\n",
    "            totalWordsCount += 1\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "        \n",
    "print(\"stop\", stopWordsCount, sep=\"\\t\")\n",
    "print(\"total\", totalWordsCount, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yslvpwpfS80P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "current_key = None\n",
    "word_sum = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "        if current_key != key:\n",
    "            if current_key:\n",
    "                print(\"reporter:counter:wiki,{0},{1}\".format(current_key, word_sum), file = sys.stderr)\n",
    "            word_sum = count\n",
    "            current_key = key\n",
    "        else:\n",
    "            word_sum += count\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "if current_key:\n",
    "    print(\"reporter:counter:wiki,{0},{1}\".format(current_key, word_sum), file = sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XptufhbMS80R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting counter_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counter.py\n",
    "\n",
    "import sys\n",
    "\n",
    "totalWordsCount = 0\n",
    "stopWordsCount = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = line.strip().split('=', 1)\n",
    "        count = int(count)\n",
    "        if key == 'stop':\n",
    "            stopWordsCount += count\n",
    "        elif key == 'total':\n",
    "            totalWordsCount += count\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "print(stopWordsCount / float(totalWordsCount) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash\n",
    "## Run map-reduce and counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bG3omEqzS80S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"stopWordsPercentage\"$(date +\"%s%6N\")\n",
    "LOGS=\"stderr_logs.txt\"\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"StopWordsPercentage\" \\\n",
    "    -files mapper.py,reducer.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    #redirect standard error to LOGS\n",
    "    -output ${OUT_DIR} > /dev/null 2> $LOGS\n",
    "    \n",
    "cat $LOGS | python3 ./counter.py\n",
    "cat $LOGS >&2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "702_to_students.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
